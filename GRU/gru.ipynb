{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gru.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RKwUedd0ojVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrTpwLOHojVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score,accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import GRU, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from keras import layers, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Nfbtx28rRVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1217
        },
        "outputId": "093e35fe-003a-418f-e4dc-8e94c12001c1"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install hyperas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.4.1)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.4.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.12.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.4.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (6.0.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.2.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.4.2)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K    100% |████████████████████████████████| 337kB 26.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (40.9.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: hyperas, prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "Successfully installed hyperas-0.4.1 prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fCT1mIKErqZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1074
        },
        "outputId": "1a9622d4-3b4a-45aa-e6ce-4d204dfdd09e"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install git+https://github.com/maxpumperla/hyperas.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/maxpumperla/hyperas.git\n",
            "  Cloning https://github.com/maxpumperla/hyperas.git to /tmp/pip-req-build-ym0r1e2g\n",
            "Requirement already satisfied (use --upgrade to upgrade): hyperas==0.4.1 from git+https://github.com/maxpumperla/hyperas.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas==0.4.1) (2.2.4)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas==0.4.1) (0.1.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas==0.4.1) (0.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas==0.4.1) (1.0.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas==0.4.1) (4.4.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas==0.4.1) (5.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (1.0.7)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (1.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas==0.4.1) (1.16.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas==0.4.1) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas==0.4.1) (4.28.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas==0.4.1) (2.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas==0.4.1) (3.8.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas==0.4.1) (7.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas==0.4.1) (6.0.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas==0.4.1) (4.4.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas==0.4.1) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas==0.4.1) (4.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas==0.4.1) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas==0.4.1) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas==0.4.1) (4.3.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas==0.4.1) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (2.10.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (2.1.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (3.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (1.4.2)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas==0.4.1) (0.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas==0.4.1) (4.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas==0.4.1) (3.4.2)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas==0.4.1) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas==0.4.1) (5.2.4)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas==0.4.1) (2.0.9)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas==0.4.1) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas==0.4.1) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas==0.4.1) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas==0.4.1) (0.5.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas==0.4.1) (4.7.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas==0.4.1) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas==0.4.1) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas==0.4.1) (40.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas==0.4.1) (2.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas==0.4.1) (17.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas==0.4.1) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas==0.4.1) (0.6.0)\n",
            "Building wheels for collected packages: hyperas\n",
            "  Building wheel for hyperas (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-k663ho3s/wheels/27/c7/75/b70097065b73570eda25350a796d87c41cd967471a04064cc2\n",
            "Successfully built hyperas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QDuittmhojVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b4dbd669-01f0-4320-cbb2-0a9365a3c5a4"
      },
      "cell_type": "code",
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform, conditional"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2f5e7a639393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'conditional'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "q8qIM2c-t9DO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "c5a7fdad-49ff-412b-a1d7-cff21b560886"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_rodD_a6ojVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/NLP/Dataset/train_E6oV3lV.csv')\n",
        "data.shape[0] - data.dropna().shape[0]\n",
        "data_copy = data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OT54nvLojVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "bb6dScp0un5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "913c37fc-f861-4ad6-bc74-be9601f319d2"
      },
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "sCqlatYfojVv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "a0c4623f-e181-4a80-d760-1569884141a5"
      },
      "cell_type": "code",
      "source": [
        "tokenized_single_posts = [nltk.tokenize.word_tokenize(i) for i in data.tweet]\n",
        "len(tokenized_single_posts)\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@',\n",
              " 'user',\n",
              " 'when',\n",
              " 'a',\n",
              " 'father',\n",
              " 'is',\n",
              " 'dysfunctional',\n",
              " 'and',\n",
              " 'is',\n",
              " 'so',\n",
              " 'selfish',\n",
              " 'he',\n",
              " 'drags',\n",
              " 'his',\n",
              " 'kids',\n",
              " 'into',\n",
              " 'his',\n",
              " 'dysfunction',\n",
              " '.',\n",
              " '#',\n",
              " 'run']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "metadata": {
        "id": "Kx6wgMedBdeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8ee9ce9a-2d1f-4e4a-fce3-866fcc42918c"
      },
      "cell_type": "code",
      "source": [
        "print(tokenized_single_posts[0])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@', 'user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YqyqR-pkojV3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "leng = []\n",
        "for i in range(len(tokenized_single_posts)):\n",
        "    length = len(tokenized_single_posts[i])\n",
        "    leng.append(length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvoU7GSgojV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### stopwords removal"
      ]
    },
    {
      "metadata": {
        "id": "twzVwef4u40S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7c75b940-abff-4b48-91e5-98f44517bf6a"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "tDQHKY7tojWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "stp_removed = []\n",
        "for i in range (len(tokenized_single_posts)):\n",
        "    stp = [word for word in tokenized_single_posts[i] if word not in (stopwords.words('english')+list(string.punctuation))]\n",
        "    stp_removed.append(stp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B72rZxyVojWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97af4154-3db9-4a76-9dcd-9968345bae0f"
      },
      "cell_type": "code",
      "source": [
        "print(len(stp_removed))\n",
        "print(stp_removed[0])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31962\n",
            "['user', 'father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zlQsyxpSojWN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lemmatize the stop removed posts"
      ]
    },
    {
      "metadata": {
        "id": "ycFaoKT0vf2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "17bed81f-3344-48d8-de11-9cce0309cbe2"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "UUwmc3oTojWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_lemma = []\n",
        "lemma = nltk.WordNetLemmatizer()\n",
        "for i in range(len(stp_removed)):\n",
        "    words = [lemma.lemmatize(word) for word in stp_removed[i]]\n",
        "    words_lemma.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0y-HBS2ojWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f138001a-1d7d-4b65-abab-38d17d4b1fc3"
      },
      "cell_type": "code",
      "source": [
        "print(len(words_lemma))\n",
        "print(words_lemma[0])"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31962\n",
            "['user', 'father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mAK3gKzmojWd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove all digit words"
      ]
    },
    {
      "metadata": {
        "id": "PgtMb70xojWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_noNum = []\n",
        "for i in range(len(words_lemma)):\n",
        "    words = [word for word in words_lemma[i] if word.isdigit() == False]\n",
        "    words_noNum.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_Xjne6hojWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f1b462b-c0b9-420f-bf39-9299332933f9"
      },
      "cell_type": "code",
      "source": [
        "len(words_noNum)\n",
        "print(words_noNum[0])"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['user', 'father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6hkQCk29ojWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove single character words"
      ]
    },
    {
      "metadata": {
        "id": "QHDfso6-ojWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_nonSingle = []\n",
        "for i in range(len(words_noNum)):\n",
        "    words = [word for word in words_noNum[i] if len(word) > 1]\n",
        "    words_nonSingle.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFXZg0J9ojWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "428664cf-3dfa-4527-fbc5-51b477193876"
      },
      "cell_type": "code",
      "source": [
        "len(words_nonSingle)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "zCgZ2vsIojW6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remove non-alphabetic words"
      ]
    },
    {
      "metadata": {
        "id": "37xdOC53ojW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_alpha = []\n",
        "for i in range(len(words_nonSingle)):\n",
        "    words = [word for word in words_nonSingle[i] if word.isalpha()]\n",
        "    words_alpha.append(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-V3stkaojXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "294c87b6-2e9c-4647-922c-5a675b6e8207"
      },
      "cell_type": "code",
      "source": [
        "print(len(words_alpha))\n",
        "print(words_alpha[0])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31962\n",
            "['user', 'father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Z0egXsbojXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_copy['words_count'] = [len(i) for i in words_alpha]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jjK46C2OwR58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "18a37d60-36f8-4404-ca61-dccd81fc3aef"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "i__VNp6CojXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "noun_freq = []\n",
        "verb_freq = []\n",
        "adjective_freq = []\n",
        "adverb_freq = []\n",
        "for i in range(len(words_alpha)):\n",
        "    word_pos_tag = nltk.pos_tag(words_alpha[i])\n",
        "    count_noun = 0\n",
        "    count_verb = 0\n",
        "    count_adjective = 0\n",
        "    count_adverb = 0\n",
        "    for j in range(len(word_pos_tag)):\n",
        "        if word_pos_tag[j][1] == \"NN\":\n",
        "            count_noun += 1\n",
        "        if word_pos_tag[j][1] == 'VB':\n",
        "            count_verb += 1\n",
        "        if word_pos_tag[j][1] == 'JJ':\n",
        "            count_adjective += 1\n",
        "        if word_pos_tag[j][1] == 'RB':\n",
        "            count_adverb += 1\n",
        "    noun_freq.append(count_noun/(len(words_alpha[i]) + 1))\n",
        "    verb_freq.append(count_verb/(len(words_alpha[i])+1))\n",
        "    adjective_freq.append(count_adjective/(len(words_alpha[i])+1))\n",
        "    adverb_freq.append(count_adverb/(len(words_alpha[i])+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "macvpusmojXR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq_dict = {'noun_freq' : noun_freq, 'verb_freq' : verb_freq, 'adjective_freq' : adjective_freq, 'adverb_freq' : adverb_freq}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1jvbthTojXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_copy = data_copy.join(pd.DataFrame(freq_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wK399OlHojXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "775da18b-6df1-4a0d-ca26-357d1f61e2a1"
      },
      "cell_type": "code",
      "source": [
        "data_copy = data_copy.drop(['id'],axis=1)\n",
        "data_copy.tail()\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>words_count</th>\n",
              "      <th>noun_freq</th>\n",
              "      <th>verb_freq</th>\n",
              "      <th>adjective_freq</th>\n",
              "      <th>adverb_freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31957</th>\n",
              "      <td>0</td>\n",
              "      <td>ate @user isz that youuu?ðððððð...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31958</th>\n",
              "      <td>0</td>\n",
              "      <td>to see nina turner on the airwaves trying to...</td>\n",
              "      <td>14</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31959</th>\n",
              "      <td>0</td>\n",
              "      <td>listening to sad songs on a monday morning otw...</td>\n",
              "      <td>8</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31960</th>\n",
              "      <td>1</td>\n",
              "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
              "      <td>8</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31961</th>\n",
              "      <td>0</td>\n",
              "      <td>thank you @user for you follow</td>\n",
              "      <td>3</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label                                              tweet  words_count  \\\n",
              "31957      0  ate @user isz that youuu?ðððððð...            4   \n",
              "31958      0    to see nina turner on the airwaves trying to...           14   \n",
              "31959      0  listening to sad songs on a monday morning otw...            8   \n",
              "31960      1  @user #sikh #temple vandalised in in #calgary,...            8   \n",
              "31961      0                   thank you @user for you follow              3   \n",
              "\n",
              "       noun_freq  verb_freq  adjective_freq  adverb_freq  \n",
              "31957   0.600000   0.200000        0.000000          0.0  \n",
              "31958   0.400000   0.066667        0.133333          0.0  \n",
              "31959   0.666667   0.000000        0.111111          0.0  \n",
              "31960   0.666667   0.000000        0.111111          0.0  \n",
              "31961   0.750000   0.000000        0.000000          0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "xRCSGkzqGn1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "56c7238e-30b2-4dbe-80e9-3df0e065667e"
      },
      "cell_type": "code",
      "source": [
        "features_distrbn = data_copy.groupby(['label'], as_index = False)['words_count', 'adjective_freq', 'noun_freq', 'adverb_freq', 'verb_freq'].mean()\n",
        "np.round(features_distrbn, 3)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>words_count</th>\n",
              "      <th>adjective_freq</th>\n",
              "      <th>noun_freq</th>\n",
              "      <th>adverb_freq</th>\n",
              "      <th>verb_freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7.785</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.368</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  words_count  adjective_freq  noun_freq  adverb_freq  verb_freq\n",
              "0      0        7.785           0.162      0.446        0.050      0.032\n",
              "1      1        8.368           0.180      0.436        0.049      0.025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "metadata": {
        "id": "VH6d05-2ojXg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### read the GLoVE 100 file for preparing an Embedding layer of 100 dimenions"
      ]
    },
    {
      "metadata": {
        "id": "MlcN9Z3OxaV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3Ul8DpiojXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5972abc-4035-4bb8-eada-88ea2ac0089a"
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "MAX_NUM_WORDS = 10000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(\"/content/glove.6B.100d.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aT9Vlu4wojXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initialize the Embedding layer with pre-trained weights from the GLoVE model:"
      ]
    },
    {
      "metadata": {
        "id": "6u6uoO2RojXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define the sequence lengths, max number of words and embedding dimensions\n",
        "MAX_SEQUENCE_LENGTH = 50   # Sequence length of each sentence. If more, crop. If less, pad with zeros\n",
        "MAX_NB_WORDS = 10000        # Top 10000 frequently occuring words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YCAa9ShNojXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train-validation-test split. We separate validation data to use early stopping callback feature during training our model below"
      ]
    },
    {
      "metadata": {
        "id": "OkmtWUWDojXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0826fff-6bcb-43bd-8e06-8a66c539c167"
      },
      "cell_type": "code",
      "source": [
        "X = data_copy.iloc[:,1:7]\n",
        "y = data_copy['label']\n",
        "print(len(X),len(y))\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31962 31962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OVaXFlc_ojXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_val, x_test, y_train_val, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WqLkUFhc0Y6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, \n",
        "                                    y_train_val, test_size = 0.2, random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wlizcka71g2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "rebuild the full training data-frame to double-check that word count and POS tagging remain useful features across the training data set as well. Note above when we analysed the test data we did not include any labels data or standardization measures, thereby strictly ensuring no leakage of test information into the trainnig space"
      ]
    },
    {
      "metadata": {
        "id": "uMu87ILmojX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "2fa8059a-728e-4fdb-81e2-08956c4d86a2"
      },
      "cell_type": "code",
      "source": [
        "train_full = pd.concat([x_train, y_train], axis = 1)\n",
        "train_full.head()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>words_count</th>\n",
              "      <th>noun_freq</th>\n",
              "      <th>verb_freq</th>\n",
              "      <th>adjective_freq</th>\n",
              "      <th>adverb_freq</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4574</th>\n",
              "      <td>here's a ?for the day the government and state...</td>\n",
              "      <td>12</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6142</th>\n",
              "      <td>cardiff!!!! ó¾©. #cardiff #euro2016 #euro #fo...</td>\n",
              "      <td>9</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22578</th>\n",
              "      <td>@user where on eah is pastor #mmusimaimane? is...</td>\n",
              "      <td>10</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3709</th>\n",
              "      <td>@user and the panel is seated! @user @user @u...</td>\n",
              "      <td>8</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5766</th>\n",
              "      <td>so often we think our country has come so far ...</td>\n",
              "      <td>9</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  words_count  \\\n",
              "4574   here's a ?for the day the government and state...           12   \n",
              "6142   cardiff!!!! ó¾©. #cardiff #euro2016 #euro #fo...            9   \n",
              "22578  @user where on eah is pastor #mmusimaimane? is...           10   \n",
              "3709    @user and the panel is seated! @user @user @u...            8   \n",
              "5766   so often we think our country has come so far ...            9   \n",
              "\n",
              "       noun_freq  verb_freq  adjective_freq  adverb_freq  label  \n",
              "4574    0.692308        0.0        0.000000     0.000000      1  \n",
              "6142    0.800000        0.0        0.000000     0.000000      0  \n",
              "22578   0.636364        0.0        0.181818     0.090909      1  \n",
              "3709    0.444444        0.0        0.222222     0.000000      0  \n",
              "5766    0.300000        0.0        0.100000     0.200000      0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "metadata": {
        "id": "2wTqA_y31oI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "cb581d5c-20aa-4e90-db2f-f5f8d4cd961e"
      },
      "cell_type": "code",
      "source": [
        "word_count_distrbn_train = train_full.groupby(['label'], \n",
        "        as_index = False)['words_count', 'adjective_freq', 'noun_freq', 'adverb_freq', 'verb_freq'].mean()\n",
        "np.round(word_count_distrbn_train, 3)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>words_count</th>\n",
              "      <th>adjective_freq</th>\n",
              "      <th>noun_freq</th>\n",
              "      <th>adverb_freq</th>\n",
              "      <th>verb_freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7.751</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.268</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  words_count  adjective_freq  noun_freq  adverb_freq  verb_freq\n",
              "0      0        7.751           0.162      0.446         0.05      0.031\n",
              "1      1        8.268           0.177      0.432         0.05      0.025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "Qn3LB0mF1uYn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "94570376-ed97-4029-96fe-8c37521f3963"
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape, x_val.shape, x_test.shape)\n",
        "print(y_train.shape, y_val.shape, y_test.shape)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17898, 6) (4475, 6) (9589, 6)\n",
            "(17898,) (4475,) (9589,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w1IuJwhX3OXK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The train-validation-test original texts are tokenized and padded for machine-readable formatting"
      ]
    },
    {
      "metadata": {
        "id": "cVFNLC4u3U0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "2992d5f3-6525-4c2f-f785-2e88fc300b51"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)   # get the frequently occuring words\n",
        "tokenizer.fit_on_texts(x_train.tweet)           \n",
        "train_sequences = tokenizer.texts_to_sequences(x_train.tweet)\n",
        "val_sequences = tokenizer.texts_to_sequences(x_val.tweet)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test.tweet)\n",
        "\n",
        "word_index = tokenizer.word_index               # dictionary containing words and their index\n",
        "#print(tokenizer.word_index)                   # print to check\n",
        "print('Found %s unique tokens.' % len(word_index)) # total words in the corpus\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "val_data = pad_sequences(val_sequences, maxlen = MAX_SEQUENCE_LENGTH)# get only the top frequent words on train\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)   # get only the top frequent words on test\n",
        "\n",
        "print(train_data[0])\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 31519 unique tokens.\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0  944    4    9    2   19    2 1477    7 1087 1854   65 2520\n",
            " 3200  666 3201  412 3629    8 1329   20]\n",
            "(17898, 50)\n",
            "(4475, 50)\n",
            "(9589, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pgy6Vh_eJSdT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "standardize all the numeric features generated above by by fitting them on training data and using that to transform validation and test data respectively\n"
      ]
    },
    {
      "metadata": {
        "id": "JBO_5xvcJVJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scaleable_cols = ['words_count', 'adjective_freq', 'noun_freq', 'adverb_freq', 'verb_freq']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "neV_czZVJm_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "858e5670-52fa-4ad0-8d2c-0b27bf43ba49"
      },
      "cell_type": "code",
      "source": [
        "scaler_multicol = MinMaxScaler()\n",
        "train_multicol_scaled = scaler_multicol.fit_transform(x_train[scaleable_cols])\n",
        "val_multicol_scaled = scaler_multicol.fit_transform(x_val[scaleable_cols])\n",
        "test_multicol_scaled = scaler_multicol.fit_transform(x_test[scaleable_cols])"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwdAMxrtJ8ba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10b5c69b-2d2f-4b77-99da-0d57720f45a4"
      },
      "cell_type": "code",
      "source": [
        "train_multicol_scaled[2]"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.27027027, 0.22727273, 0.65356265, 0.12121212, 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "metadata": {
        "id": "GPv8yUtTKFnc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7cf5d92a-b0ac-4c21-c23b-0aa671454683"
      },
      "cell_type": "code",
      "source": [
        "train_data = np.hstack((train_data, train_multicol_scaled))\n",
        "val_data = np.hstack((val_data, val_multicol_scaled))\n",
        "test_data = np.hstack((test_data, test_multicol_scaled))\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17898, 55)\n",
            "(4475, 55)\n",
            "(9589, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zenq_yPSKoNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "009c4709-7c97-40b9-952e-902b95222eb7"
      },
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 9.44000000e+02 4.00000000e+00\n",
            " 9.00000000e+00 2.00000000e+00 1.90000000e+01 2.00000000e+00\n",
            " 1.47700000e+03 7.00000000e+00 1.08700000e+03 1.85400000e+03\n",
            " 6.50000000e+01 2.52000000e+03 3.20000000e+03 6.66000000e+02\n",
            " 3.20100000e+03 4.12000000e+02 3.62900000e+03 8.00000000e+00\n",
            " 1.32900000e+03 2.00000000e+01 3.24324324e-01 0.00000000e+00\n",
            " 7.11018711e-01 0.00000000e+00 0.00000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M-UkgyTGL9N4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "initialize the embedding matrix from GLoVE 100 which will be used as the initial weights of the Embedding layer in our neural network"
      ]
    },
    {
      "metadata": {
        "id": "PGcKcltqKuL_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = min(MAX_NB_WORDS, len(embeddings_index))\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_NB_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VQAjA0ujNCVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cell declares our final model that has:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   an initial Embedding layer with pre-trained weights from GLoVE 100 as described above, hence trainable = False at this layer,\n",
        "*   two layers of GRU model with in-between BatchNormalization and Dropout as mentioned\n",
        "*   the neural network uses the Keras function API and is then split after the second GRU layer into a one output model - 'label',\n",
        "*   the output model has two Dense() layers as we want to visualize the learning of the network just prior to it's final predictions\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ltd8golNLu86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "1e5660ab-9b7b-41f0-d97b-2603baeb2d5d"
      },
      "cell_type": "code",
      "source": [
        "posts_input = Input(shape=(None,), dtype='int32', name='all_posts')\n",
        "embedded_posts = Embedding(input_dim=MAX_NB_WORDS,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH, \n",
        "                            output_dim=EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=False)(posts_input)\n",
        "\n",
        "x = layers.GRU(128, activation='relu', return_sequences = True)(embedded_posts)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.GRU(64, activation = 'relu')(x)\n",
        "x = layers.Dense(16, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "label_pred = layers.Dense(8, activation = 'relu', name = 'label0')(x)\n",
        "label_pred = layers.Dropout(0.5)(label_pred)\n",
        "label_pred = layers.Dense(1, activation = 'sigmoid', name = 'label1')(label_pred)\n",
        "\n",
        "\n",
        "combined_model = Model(posts_input, [label_pred])\n",
        "combined_model.summary()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "all_posts (InputLayer)       (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 50, 100)           1000000   \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 50, 128)           87936     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 50, 128)           512       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 64)                37056     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                1040      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "label0 (Dense)               (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "label1 (Dense)               (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 1,126,689\n",
            "Trainable params: 126,433\n",
            "Non-trainable params: 1,000,256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n1CtLxGkPeiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "callbacks_list = [EarlyStopping(monitor='val_loss', patience=1, ),\n",
        "                ModelCheckpoint(filepath='model_multi-feature.h5', monitor='val_loss',\n",
        "                        save_best_only=True,)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EG3t8OY3QZT5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "combined_model.compile(optimizer = 'rmsprop',loss = {'label1' : 'binary_crossentropy'},metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UOEAvDNvRi9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "d87df6db-68a5-464b-aa27-7aebc933899f"
      },
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4574     1\n",
              "6142     0\n",
              "22578    1\n",
              "3709     0\n",
              "5766     0\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "metadata": {
        "id": "Q2Zvd-FdSKOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 16\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yvCmN-euQzRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "2ea93371-ebab-40f3-c098-5135a322ca3a"
      },
      "cell_type": "code",
      "source": [
        "hist = combined_model.fit(train_data, {'label1' : y_train},\n",
        "                                  epochs = epochs, batch_size = batch_size,\n",
        "                   callbacks = callbacks_list,\n",
        "               validation_data = (val_data, {'label1' : y_val})).history"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 17898 samples, validate on 4475 samples\n",
            "Epoch 1/16\n",
            "17898/17898 [==============================] - 78s 4ms/step - loss: 0.2876 - acc: 0.9151 - val_loss: 0.1718 - val_acc: 0.9280\n",
            "Epoch 2/16\n",
            "17898/17898 [==============================] - 76s 4ms/step - loss: 0.2168 - acc: 0.9327 - val_loss: 0.1531 - val_acc: 0.9457\n",
            "Epoch 3/16\n",
            "17898/17898 [==============================] - 77s 4ms/step - loss: 0.1857 - acc: 0.9412 - val_loss: 0.2215 - val_acc: 0.9457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "509Mzz6MSFPx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1 = load_model('model_multi-feature.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZSy7KslVbUq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = model1.predict(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uN9SsGqdV9df",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_list = list(chain.from_iterable(pred))\n",
        "label_predict = [1 if x >= 0.5 else 0 for x in label_list]\n",
        "label_recall = recall_score(label_predict, y_test)\n",
        "label_acc = accuracy_score(label_predict, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIe0pYyJXHF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cbc33c75-69cb-4082-a88d-68a02019fbbf"
      },
      "cell_type": "code",
      "source": [
        "print(\"ACCURACY : \",label_acc)\n",
        "\n",
        "print(\"Recall : \",label_recall)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY :  0.9465011992908541\n",
            "Recall :  0.7821428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0dDWyW7BXDcj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}